---
title: "Using machine learning algorithms to build artificial intelligence-based movie recommendation systems"
author: "Sean Cranston"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    number_sections: true
    theme: lumen
---

# Acknowledgements

This paper and the research behind it would never have happened without the support of my supervisor, Dr. Mengshi Zhou, and Lois Schindler, the person who introduced me to Dr. Zhou. Without Dr. Zhou's enthusiasm, knowledge and willingness to work with me, this research paper would have never happened. And without the support of Lois, this research opportunity would have never crossed my desk. Dr. Zhou and Lois have been a great pillar of support and I am grateful for them.

# Introduction

## Recommendation engines

The fast development of internet technology has led to the explosive growth of available data in the past few years. However, the huge amount of data is often an obstacle to finding relevant information due to the present of spam data or erroneous information.  

Recommendation systems can solve the information overload problem by guiding people towards the information according to their preferences. The goal of a recommendation engine is to assist the user - like a consumer - in discovering information that is relevant to them. For example, <http://google.com> uses a recommendation engine. After you search something, those links are the top recommendations that the algorithm gives to the user. And Recommendations are not just for Google. They can be found in online marketing, online dating, book and music recommendations, and a plethora of other areas. For this article we will be focusing on movie recommendations. Due to the COVID-19 pandemic, consumers relied on video-streaming service such as Netflix and Disney+. For instance, think of Netflix. How does it recommend movies from its large movie database to its even larger costumer base? This will be the question we investigate throughout this article.

In this study, we investigate the performance of different recommendation algorithms on a commonly used movie recommendation dataset. We then build an artificial intelligence (AI)-based recommendation using the algorithm with the best performance. Finally we develop a shiny app for our AI-based recommendation engine.

## Machine learning algorithms fo recommendation engines

For this study we will be looking at 9 different recommendation algorithms. They are (along with a brief description):

* __User-Based Content filtering Cosine (UBCF_C):__ Items are recommended assuming that similar users will rate items similarly. Based on a Cosine similarity matrix from a matrix that has movies as columns and users as rows.

* __Item-Based Content Filtering Cosine (IBCF_C):__ Items are recommended assuming that they will prefer other items which are similar to items they like. Based on the same similarity matrix from UBCD_C.

* __User-Based Content filtering Pearson (UBCF_P):__ The idea with UBCF_P is the same as UBCF_C. But instead of using the cosine similarity matrix, we use the Pearson similarity matrix.

* __Item-Based Content Filtering Pearson (IBCF_P):__ The idea with IBCF_P is the same as IBCF_C. But instead of using the cosine similarity matrix, we use the Pearson similarity matrix.

* __Singular Value Decomposition (SVD):__ SVD is a factorization of a real matrix. It then extract the latent factors and uses the latent factor matrix to make recommendations.

* __Popular:__ This method simply recommends based on item/movie popularity.

* __Alternating Least Squares Explicit (ALS):__ Recommends for explicit ratings based on latent factors, calculated by alternating least squares algorithm.

* __Alternating Least Squares Implicit (ALS_implicit):__ Recommends for implicit data based on latent factors, calculated by alternating least squares algorithm.

* __Random:__ Produce random recommendations (so we have a baseline to judge the other models with).


# Data description

We obtained our data from IBMT, Which is an online database of information related to television programs, home videos, video games, and more importantly for us  films. The data can be obtained from <https://drive.google.com/file/d/1Dn1BZD3YxgBQJSIjbfNnmCFlDW2jdQGD/view>. There are two data frames. One is a key comprising of a movie ID and the title of the unique movie that goes to that ID including the genre associated with the movie. The other data frame is the data we will be doing our analysis on, which comprises of userID, MovieID and the rating the user gave to the movie. 

# Data preprocessing

In our analysis we will make use of 5 packages *tidyverse, data.table, reshape2,lobstr, and recommenderlab*. 


```{r setup}
knitr::opts_chunk$set(echo = TRUE,
                      warning = F,
                      message = F)
library(tidyverse)                   
library(data.table)
library(reshape2)
library(recommenderlab)
library(lobstr)
```

## Data importing

You could save the data from <https://drive.google.com/file/d/1Dn1BZD3YxgBQJSIjbfNnmCFlDW2jdQGD/view> and then upload it from the save location. But for this analysis, to ease the Reproducibility, we save the data on the github page <https://github.com/SeanCranston/2021_Summer_Research> under the Data folder. This way we don't have to download and save the file. We can just put in the URL to get the data (it does download to a temp file but is not permanent).  

```{r Data_import}
movie_data <- read_csv("https://raw.githubusercontent.com/SeanCranston/2021_Summer_Research/master/Data/movies.csv",
              col_types = cols(
                         movieId = col_double(),
                         title = col_character(),
                         genres = col_character()
                       ))
head(movie_data)
rating_data <- read_csv("https://raw.githubusercontent.com/SeanCranston/2021_Summer_Research/master/Data/ratings.csv",
                        col_types = cols(
                          userId = col_double(),
                          movieId = col_double(),
                          rating = col_double(),
                          timestamp = col_double()
                        ))
head(rating_data)
```


## Cleaning genre matrix

In this code chunk we want to clean the `movie_data` data frame. First, we need to find all the different genres in our data set and put it into a vector string. We will use this vector to split the different genres for the same movies. From there we will create a new matrix called `genre` with column names labeled with the vector of genres. Then we will put `genre` in a for loop and indicate with a `1` whether a movie has a specific genre. Lastly, we will bind the movies with the `genre` matrix for an easy reference. 

```{r genre_matrix}
# different genres
names_genre <- c("Action", "Adventure", "Animation", "Children", "Comedy", "Crime",
           "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical", 
           "Mystery","Romance", "Sci-Fi", "Thriller", "War", "Western") 

# split different genres for the same movie
list_genre <- str_split(movie_data$genres, "[|]")
head(list_genre)

# create genre matrix
genre = matrix(0, nrow(movie_data), ncol = length(names_genre)) %>% 
    `colnames<-`(names_genre) 
head(as_tibble(genre))
    
# fill genre matrix with 1 if the movie has that genre
for (row in 1:nrow(genre)) {
    for (col in 1:ncol(genre)) {
        genre[row,col] = ifelse(any(names_genre[col] == list_genre[[row]]), 1, 0)
    }
}

genre = as.data.frame(genre)
str(genre)

SearchMatrix <- cbind(movie_data[,1:2], genre[])
head(SearchMatrix, 10)
```

So from here we have a matrix that has the movie and the genre as variables. A better to have a matrix for computations.

## Cleaning rating matrix

As of right now we have our `rating_data` in the format of where each row comprises of a user, movie, and a rating. `rating_data` also contains a timestamp column, but we will ignore that for this analysis. What we want is the data to be in the format where each row is a user, each column is a movie, and the entry for each user movie pair is a rating. In order to accomplish this we use *reshape2* `dcast()` function. Next, we delete the userID column since the row index is the same number as that column. From here we have a huge matrix with 668 rows (users) and 10325 movies. Note that most of the rating entries are also NA (not rated), we will show this is the case in our data visualization section, though it can be seen by the size of the different matrices. 

```{r rating_matrix}
# change to user vs movie matrix with rating as the entry
ratingMatrix <- reshape2::dcast(rating_data, 
                                userId~movieId, 
                                value.var = "rating", 
                                na.rm=FALSE) %>% 
    select(-c('userId')) %>% # same as row index, just taking up space
    as.matrix() %>% #needs to be matrix to convert to sparse matrix
    as(.,"realRatingMatrix") 

# size of rating_data (can't really use this to make a predicting model)
obj_size(rating_data)

# size of the rating matrix when not in sparse format
obj_size(rating_data %>% 
           dcast(userId~movieId,value.var = "rating",na.rm=F) %>% 
           select(-c('userId')) %>% 
           as.matrix())

#size of the sparse matrix
obj_size(ratingMatrix)

#structure of our sparse matrix
str(ratingMatrix)

#other representations of ratinMatrix
dim(ratingMatrix)
#summary(ratingMatrix@data) %>% head() # how to you should visualize sparse matrix
```

Note that our `ratingMatrix` is of class s4 which just means it has slots (those are the what the @ are in `str(ratingMatrix)`. For more information I welcome you to read <https://adv-r.hadley.nz/s4.html?q=s4#s4> which does a very good job at describing what its purpose in object orientating programming. Note that putting our `ratingMatrix` reduces the size of our object by (55,838,040-1,968,424)/55,838,040 or `r (55838040-1968424)/55838040`%. Which makes storing and using are data frame much more efficient. 

# Data Visualization

## How many unique users/movies

unique users: `r dim(ratingMatrix)[1]`

unique movies: `r dim(ratingMatrix)[2]`

## similar matrix of 100 users based on users

Below in the matrix, each row and column represents an user. We took ten users and found how similar they are by using the Cosine similarity matrix method. For visualizations we also added a colored plot.

```{r}
similarity_mat <- similarity(ratingMatrix[1:10, ], #get 10 users 
                             method = "cosine",
                             which = "users") %>%  #similar based on user
  as.matrix()
similarity_mat
image(similarity_mat, main = "User's Similarities")

```

## similar matrix of 100 users based on item

Similar to the above matrix, in the matrix below each row and column represents an item. We took ten items and found how similar they are by using the Cosine similarity matrix method. For visualizations we also added a colored plot.

```{r}
movie_similarity <- similarity(ratingMatrix[,1:10],
                               method = "cosine",
                               which = "items") %>%  #similar based on movie
    as.matrix()
movie_similarity
image(movie_similarity, main = "Movies similarity")
```

## distribution of ratings

Below is a table and two graphs. The main thing to notice here is that most movies do not have a rating. That is, we are working with very sparse data.

```{r}
table_of_ratings <- as.vector(ratingMatrix@data) %>% 
    table()
table_of_ratings

ratings <- table_of_ratings %>% as.data.frame()
# with non rating movies
ggplot(ratings,
       aes(x = ., y = Freq)) +
  geom_col()
#only with rated movies
ggplot(ratings[-c(1),],
       aes(x = ., y = Freq)) +
  geom_col()
```

## Most Popular Movies

For fun we can explore which movies are the most popular in our data set. To do that we just count the number of views (ratings) a movie has and then organize them in a user friendly table and graph as shown below.

```{r}
table_view <- ratingMatrix %>% 
  colCounts() %>% 
  data.frame(movie = names(.),
             views = .) %>% 
  arrange(desc(views)) %>% 
  mutate(title = NA)
for (index in 1:10325){ #add title to table_views
  table_view[index,3] <- as.character(
    subset(movie_data,
           movie_data$movieId == table_view[index,1])$title)
}
table_view[1:6,]

ggplot(table_view[1:6,], aes(x = title, y = views))+
    geom_bar( stat="identity",fill = 'steelblue') +
    geom_text(aes(label=views), vjust=-0.3, size=3.5) +
    theme_bw()+
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    ggtitle("Total Views of the Top Films")+
    ylim(0, 350)
```



## Heatmap of first 25 rows and columns

The heat map below is another visual to show us how sparse our data is. Note that each row is a user and each column is a movie with the rating being the colored box.

```{r}
# Heatmap of movie ratings
image(ratingMatrix[1:25,1:25], 
      axes = FALSE, 
      main = "Heatmap of the first 25 rows and 25 columns")

```

# Data Preparation

As we could see from section 4.4, there are many unrated movies by users. To make computations easier it is best practice to pick the movies and users with a certain threshold to be used in our model. For instance if a movie only has one rating it would be hard to recommend that movie to others. likewise if a user only rates one movie it would be hard to build a profile for that user. For those reasons we make it a requirement for our data that each user must rate more than 50 movies and each movie we include in our data set will need to be rated by more than 50 people

## reducing `ratingMatrix`

We reduce our matrix by getting rid of the users that rated fewer that 50 movies and movies that were rated by fewer than 50 users.

```{r}
movie_ratings <- ratingMatrix[rowCounts(ratingMatrix) > 50,
                              colCounts(ratingMatrix) > 50]
dim(movie_ratings)
dim(ratingMatrix) #got rid of a lot of movies
```

## visualizing reduced matrix
From the above output of `movie_ratings`, we observe that there are 420 users and 447 films as opposed to the previous 668 users and 10325 films. We can now observe our matrix of relevant users as follows:

```{r}
minimum_movies<- quantile(rowCounts(movie_ratings), 0.98)
# 98th percentile of how many movies a user rated
minimum_users <- quantile(colCounts(movie_ratings), 0.98)
image(movie_ratings[rowCounts(movie_ratings) > minimum_movies,
                    #we only want users that rated more movies than the lower 999%
                    colCounts(movie_ratings) > minimum_users],
      main = "Heatmap of the top users and movies")
```

Where we are looking at the users who rated the most movies vs the movies that where rated the most.

Next, we can look at the distribution of the average rating per user.

```{r}
average_ratings <- rowMeans(movie_ratings)
qplot(average_ratings, fill=I("steelblue"), col=I("white")) +
    ggtitle("Distribution of the average rating per user")
```

## Data Normalization

We want to have normalized data because not everyone rates the same way. For example, we could have a happy user who rates all movies between 4 and 5 stars. Compared to a mean user who might never rate a movie higher than a 3. Normalizing our data will allow us to compare the mean and friendly raters without bias.

```{r}
normalized_ratings <- normalize(movie_ratings)
sum(rowMeans(normalized_ratings) > 0.1)

image(normalized_ratings[rowCounts(normalized_ratings) > minimum_movies,
                         colCounts(normalized_ratings) > minimum_users],
      main = "Normalized Ratings of the Top Users")

```

## Data Binarization

Instead of normalizing the data we could pick our own threshold to determine which movie gets a 0 or 1. For instance, we could set a threshold at 3 such that when a movie is rated a 3 or lower it gets a new rating of 0 and if higher than 3 it gets a new rating of 1.

```{r}
binary_minimum_movies <- quantile(rowCounts(movie_ratings), 0.98)
#I think they changed this to .98 so their would be more white spaces
binary_minimum_users <- quantile(colCounts(movie_ratings), 0.98)
#movies_watched <- binarize(movie_ratings, minRating = 1)
good_rated_films <- binarize(movie_ratings, minRating = 3)
image(good_rated_films[rowCounts(movie_ratings) > binary_minimum_movies,
                       colCounts(movie_ratings) > binary_minimum_users],
      main = "Heatmap of the top users and movies")
```

## Quick Note

It is best to use Normalization and binarization together. That is, we want to normalize each row and then binarize the data by row. However, the package *recommenderlab* does this for us so we don't need to worry about writing the code.

# A list of machine learning models in *recommenderlab*

```{r model_options}
# Model options
recommendation_model <- recommenderRegistry$get_entries(
  dataType = "realRatingMatrix")
names(recommendation_model)

#list of model types and description
lapply(recommendation_model, "[[", "description") 

#different parameters for each model
# recommendation_model$IBCF_realRatingMatrix$parameters
# recommenderRegistry$get_entries(dataType = "realRatingMatrix")
```

# setup scheme

We will be using 3-fold cross validation. where the data is split into 3 sub data frames, and then each sub data frame is used to calculate a model. Then the final model is averaged by all the models from each sub data frame. 

```{r}
e <-  evaluationScheme(movie_ratings, #evaluate will normalize for us
                       method="cross", 
                       k = 3,#3 fold cross validation
                       given=3, 
                       goodRating=3)#>= 0 is a good rating for normalized data
e
```

# Model Parameter Optimization

In order to provide a complete picture of the performance of each algorithm, we performed the 3-fold cross-validation for different parameter combinations. The parameters were then tuned for optimal performance using Precision-Recall (PR) curves and receiver operating characteristic (ROC) curves. 

For each model the following parameters will be optimized:

Model | # of Recommendations | Parameter | Method
-|-|-|-
IBCF | 1,5,10,15,20,25 | Similar Items (k): 1,5,10,20,30,40 | Cosine, Pearson
UBCF | 1,5,10,15,20,25 | Nearest Neighbors (nn): 1,5,10,20,30,40 | Cosine
SVD | 1,5,10,15,20,25 | Item Subset (k): 1,5,10,20,30,40,100 | NA
ALS | 1,5,10,15,20,25 | Latent Factors (k): 1,5,10,20,30,40 | Implicit, Explicit
Popular| 1,5,10,15,20,25 | | NA

## Item Based Content Filtering (Cosine and Pearson)

For the Cosine method we will use k = 40 since it has the greatest area under the curve. Then for the Pearson method we will use k = 30 since it is consistently one of the best models in the ROC curve and the precision recall curve.

```{r}
#IBCF Cosine
vector_k <- c(1, 5, 10, 20, 30, 40)
models_to_evaluate <- lapply(vector_k, function(k){
  list(name = "IBCF", param = list(method = "Cosine", k = k))
})
names(models_to_evaluate) <- paste0("IBCF_k_", vector_k)

n_recommendations <- c(1, 5, 10, 15, 20, 25)
list_results <- evaluate(x = e, method = models_to_evaluate, n = n_recommendations)
plot(list_results, annotate = 1, legend = "topleft")
title("ROC curve IBCF-Cosine")
plot(list_results, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall IBCF-Cosine")

#IBCF Pearson
models_to_evaluate <- lapply(vector_k, function(k){
  list(name = "IBCF", param = list(method = "Pearson", k = k))
})
names(models_to_evaluate) <- paste0("IBCF_k_", vector_k)

n_recommendations <- c(1, 5, 10, 15, 20, 25)
list_results <- evaluate(x = e, method = models_to_evaluate, n = n_recommendations)

plot(list_results, annotate = 1, legend = "topleft")
title("ROC curve IBCF-Pearson")
plot(list_results, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall IBCF-Pearson")
```


## User Based Content Filtering (Cosine only)

UBCF with the Cosine similarity matrix is at its best with our data when nn = 1.

Note that the blocked-out code does not generate the UBCF Pearson model. The problem could lie in that UBCF Pearson is not designed to be made from a sparse matrix.

```{r}
#UBCF Cosine
vector_k <- c(1, 5, 10, 20, 30, 40)
models_to_evaluate <- lapply(vector_k, function(k){
  list(name = "UBCF", param = list(method = "Cosine", nn = k))
})
names(models_to_evaluate) <- paste0("UBCF_nn_", vector_k)

n_recommendations <- c(1, 5, 10, 15, 20, 25)
list_results <- evaluate(x = e, method = models_to_evaluate, n = n_recommendations)

plot(list_results, annotate = 1, legend = "topleft")
title("ROC curve UBCF-Cosine")
plot(list_results, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall IBCF-Cosine")

#UBCF Pearson
# models_to_evaluate <- lapply(vector_k, function(k){
#   list(name = "UBCF", param = list(method = "pearson", nn = k))
# })
# names(models_to_evaluate) <- paste0("UBCF_nn_", vector_k)
# 
# n_recommendations <- c(1, 5, 10, 15, 20, 25)
# list_results <- evaluate(x = e, method = models_to_evaluate, n = n_recommendations)
# 
# plot(list_results, annotate = 1, legend = "topleft")
# title("ROC curve UBCF-Pearson")
# plot(list_results, "prec/rec", annotate = 1, legend = "bottomright")
# title("Precision-recall UBCF-Pearson")
```

## Singular Value Decomposition

It appears that when k = 10, or we have 10 item subsets, SVD is at its best.

```{r}
#SVD
vector_k <- c(1, 5, 10, 20, 30, 40, 100)
models_to_evaluate <- lapply(vector_k, function(k){
  list(name = "SVD", param = list(k = k))
})
names(models_to_evaluate) <- paste0("SVD_k_", vector_k)

n_recommendations <- c(1, 5, 10, 15, 20, 25)

list_results <- evaluate(x = e, method = models_to_evaluate, n = n_recommendations)

plot(list_results, annotate = 1, legend = "topleft")
title("ROC curve SVD")
plot(list_results, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall SVD")
```

## Alternating Least Squares

When n = 40, ALS performs its best.

```{r}
#ALS
vector_k <- c(1, 5, 10, 20, 30, 40)
models_to_evaluate <- lapply(vector_k, function(k){
  list(name = "ALS", param = list(n_factors = k))
})
names(models_to_evaluate) <- paste0("ALS_n_", vector_k)

n_recommendations <- c(1, 5, 10, 15, 20, 25)
list_results <- evaluate(x = e, method = models_to_evaluate, n = n_recommendations)

plot(list_results, annotate = 1, legend = "topleft")
title("ROC curve ALS")
plot(list_results, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall ALS")
```

## Popular

The Popular algorithm doesn't have any parameters to compare, so there is no code and graphs to show here.

# Model Comparison

## Models with parameters

When comparing different models in *recommenderlab* (and maybe r) we can put them in a list. This allows us to compare models with ease.

```{r}
Models_to_be <- list("UBCF_C" = list(name = "UBCF", 
                                     param = list(method = "cosine",
                                                  nn = 1)),
                     "IBCF_C" = list(name = "IBCF",
                                     param = list(method = "cosine",
                                                  k = 40)),
                     # "UBCF_P" = list(name = "UBCF",#not working :(
                     #                 param = list(method = "Pearson")),
                     "IBCF_P" = list(name = "IBCF",
                                     param = list(method = "Pearson",
                                                  k = 30)),
                     "SVD" = list(name = "SVD",
                                  param = list(k = 10)),
                     "rand" = list(name = "RANDOM"),
                     "Popular" = list(name = "POPULAR"),
                     "ALS" = list(name = "ALS",
                                  param = list(n_factors = 40))
                     #"ALS_implicit" = list(name = "ALS_implicit")
                     )
```


## RMSE

As we can see from the graph below the Model with the best RMSE is the Popular algorithm. A close second is the ALS method followed by the SVD algorithm.

```{r}
r <- evaluate(e, 
              Models_to_be, 
              type = "ratings")# this gives estimated errors
rec <- Recommender(data = getData(e,"train"),
                              method = "IBCF",
                              parameter = list(k = 30))#this is defualt
rec
class(rec)
plot(r, ylim = c(0,3.25)) # RMSE
```

## ROC and precision recall curve

Then from both the graphs below we can see that the popular algorithm out performs the others.

```{r}
Models_results <- evaluate(e,
                      Models_to_be,
                      #type = "topNList",
                      n = c(1,3,5,10*1:10) #different values to generate top-N list
                      )#it doesn't like this

plot(Models_results, annotate=c(1,3), legend = "bottomright") #ROC curve
plot(Models_results, "prec/rec", annotate = 3, legend = "bottomright") #precision recall curve
```

## Other Comparisons

Note from the last graph that as the number of recommendations increase the popular algorithm finds itself having less false negatives and a higher accuracy. 

Then from the graph below note that as Accuracy ((TPR+(1-FPR))/2) goes up FP also increases. But the Popular model rate of increase far exceeds that of how much the FP is growing compared to other models.

```{r}
avg_conf <- function(results) {
  tmp <- results %>%
    getConfusionMatrix()
  as.data.frame(Reduce("+",tmp) / length(tmp))
}

Models_avg <- Models_results %>%
  map(avg_conf) %>% 
  enframe() %>% 
  unnest(cols = value) %>% 
  mutate(n = as.factor(n)) 

Models_avg

ggplot(Models_avg, aes(x = FP, y = (TPR + (1-FPR))/2, shape = name, color = n))+
  geom_point()

```

## Best Model

Looking at all our test it is obvious that from our data the popular algorithm beats the others easily. So that is the algorithm we will use in the next section on predictions

# Recommended movies for a new user using the best algorithm

## Making a model

At this time the `evaluate()` functions `keepModel` parameter doesn't seem to work. Thus we have to make our model again with the `Recommender()` function.

```{r}
rec_model <- Recommender(data = getData(e,"train"),
                         method = "POPULAR")
rec_model
class(rec_model)
```

## Model exploring

```{r}
rec_model
getModel(rec_model)
s <- getModel(rec_model)
s %>% str()
```

## Predicting

### Predicting for one user

using the model above we will predict 10 movies that the first user might like.

```{r}
top_recommendations <- 10 # the number of items to recommend to each user
predicted_recommendations <- predict(object = rec_model,
                                     newdata = getData(e,"known"),
                                     n = top_recommendations)
predicted_recommendations

# 10 recommendations for the first user
user1 <- predicted_recommendations@items[[1]] 
movies_user1 <- predicted_recommendations@itemLabels[user1]
movies_user2 <- movies_user1
for (index in 1:10){
  movies_user2[index] <- as.character(
    subset(movie_data,movie_data$movieId == movies_user1[index])$title)
}
movies_user2

```

### Predicting for multiple users

Another way to go about this is to predict for multiple users which is done by the code below.

```{r}
# matrix with the recommendations for each user
recommendation_matrix <- sapply(predicted_recommendations@items,
                                function(x){ 
                                  as.integer(colnames(movie_ratings)[x]) 
                                }) 
recommendation_matrix[,1:4]
```

## Distribution of predictions

now we explore the most recommended movies.

```{r}
sort(table(recommendation_matrix),decreasing = T)
number_of_items <- sort(table(recommendation_matrix),decreasing = T) %>% 
  as_tibble() %>% 
  filter(n > 50)
ggplot(number_of_items,aes(x = recommendation_matrix, y = n), fill = "blues")+
  geom_col(fill = "#00AFBB")+
  xlab("Movie Index")+
  ylab("Number of Times Recommended")+
  ggtitle("Distribution of the Number of Items for Popular")+ 
  theme_bw()+
  theme(plot.title = element_text(hjust = .5))
```

## Title and Number of recommendations

As we can see from the data frame below those were popular movies back in there day, and movies like Star Wars still are popular, so it makes sense that the popular algorithm would recommend them.

```{r}
number_of_items

for (i in 1:4) {
  number_of_items[i,1] <- movie_data %>% 
    filter(movieId == as.integer(number_of_items[i,1])) %>% 
    select(title)
}
colnames(number_of_items) <- c("Movie Title", "No. of Items")
number_of_items[1:4,]
```

Note that we initially made recommendations for 140 users and "Schindler's List (1993)" and "Godfather, The (1972)" was recommended 139 times! This is something that can easily become a big problem in how we recommend movies. For Example, if we always recommend the top movies how do we know if the movies that not many people have seen are actually very good movies?

## Try the app

To play around with the different models and how they make predictions go to <https://sean-cranston.shinyapps.io/Movie_recommender/>, a shiny app that will allow the user to get a feel for what type of predictions we can expect.

# Conclusion

Our exploration of recommender systems used User-Based Collaborative Filtering (UBCF), Item-Based Collaborative Filtering (IBCF), Singular Value Decomposition, Alternating Least Squares, and the popular algorithm to select movies for the user to watch next. We determined that the Popular algorithm was the most accurate model, producing higher specificity for all ranges of n.

The `Recommenderlab` package we relied on provides many more options for refining models, like Support Vector Machines. We were able to explore many of the package functions as a means of deepening our understanding of classification techniques and concepts. All the same, this small-scale study barely scratched the surface of this domain.

# References

*Funding*
NSF Grant Managed by Lois Schindler

*Advisor*
Dr. Mengshi Zhou

*Data Flair*
https://data-flair.training/blogs/data-science-r-movie-recommendation/

*Netflix top ten* 
https://sifter.org/~simon/journal/20061211.html

*Intro to sparse matrix formats*
https://www.gormanalysis.com/blog/sparse-matrix-storage-formats/
https://www.gormanalysis.com/blog/sparse-matrix-construction-and-use-in-r/

*Maruti techlabs*
https://marutitech.com/recommendation-engine-benefits/#:~:text=There%20are%20basically%20three%20important%20types%20of%20recommendation,filtering%202%20Content-Based%20Filtering%203%20Hybrid%20Recommendation%20Systems

https://rstudio-pubs-static.s3.amazonaws.com/276939_e571a5c526274fc688551066c058841e.html#comparing-multiple-models

https://rpubs.com/dhairavc/639597

And the manual for Recommenderlab which is attached to the github repository https://github.com/SeanCranston/2021_Summer_Research



